%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[apj]{emulateapj}
%\documentclass[preprint2]{aastex61}
%\documentclass[12pt,preprint]{aastex}
\graphicspath{{figures/}}
\DeclareGraphicsExtensions{.jpg,.pdf,.png,.eps,.ps}

\usepackage[table,usenames,dvipsnames]{xcolor}
%\usepackage{amsmath}
\usepackage{subfigure}
\usepackage[backref,breaklinks,colorlinks,citecolor=blue]{hyperref}
\usepackage{natbib}
%\usepackage{natbib}
\bibliographystyle{fapj}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{soul}

%\newcommand{\jcap}{JCAP}

\newcommand{\sqdeg}{deg$^2$ }
\newcommand{\omb}{\ensuremath{\Omega_b h^2}}
\newcommand{\omc}{\ensuremath{\Omega_c h^2}}
\newcommand{\clpp}{\ensuremath{C_{L}^{\phi\phi}}}
\newcommand{\cpmf}{\ensuremath{C_{\ell}^{\rm PMF}}}

\newcommand{\cpmftens}{\ensuremath{C_{\ell}^{\rm PMF,\,tens}}}
\newcommand{\cpmfvec}{\ensuremath{C_{\ell}^{\rm PMF,\,vec}}}
\newcommand{\apmf}{\ensuremath{A_{\rm PMF}}}
\newcommand{\bpmf}{\ensuremath{B_{\rm 1\,Mpc}}}
\newcommand{\alens}{\ensuremath{A_{\rm lens}}}
\newcommand{\lcdm}{\ensuremath{\Lambda}CDM}
\newcommand{\nrun}{\ensuremath{n_{\rm run}}}
\newcommand{\neff}{\ensuremath{N_{\rm eff}}}
\newcommand{\ho}{H\ensuremath{_0}}
\newcommand{\mnu}{\ensuremath{\sum m_\nu}}
\newcommand{\ukarcmin}{\ensuremath{\mu}{\rm K-arcmin}}
\newcommand{\lknee}{\ensuremath{\ell_{\rm knee}}}
\newcommand{\fermilat}{\textit{Fermi}-LAT}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\planck}{{\sl Planck}}
\newcommand{\wmap}{{\sl WMAP}}
\newcommand{\bicepkeck}{BICEP2/Keck Array}
\newcommand{\sptnew}{SPT-3G}
\newcommand{\pb}{\textsc{Polarbear}}
\newcommand{\simons}{Simons Array}
\newcommand{\sptpol}{SPTpol}
\newcommand{\advactpol}{Adv.~ACTpol}

\newcommand{\tbd}[1]{\textcolor{Red}{{\bf TBD}: #1}}
\newcommand{\gab}[1]{\textcolor{Orchid}{[{\bf GS}: #1]}}
\newcommand{\changed}[1]{\textcolor{Red}{#1}}
\newcommand{\removed}[1]{\textcolor{Red}{}}
\include{number_list}

%

% ref to section \S\ref{sec:label}

%\submitjournal{ApJ}
\def\Melbourne{1}
\def\uci{2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Cosmic Microwave Background Power Spectra from Few Bit Timestreams}
\author{L.~Balkenhol\altaffilmark{\Melbourne} and C.~L.~Reichardt\altaffilmark{\Melbourne}}
\altaffiltext{\Melbourne}{School of Physics, University of Melbourne, Parkville, VIC 3010, Australia}
\email{christian.reichardt@unimelb.edu.au}

\begin{abstract}
Observations of the Cosmic Microwave Background (CMB) are of immense value to modern cosmology. However, future CMB experiments must confront challenges in mission planning, hardware and analysis that arise from the sheer size of the time-ordered-data (TOD) being recorded. These challenges are particularly significant for Antarctic and satellite experiments which depend on satellite links to transmit their data. We investigate using extreme digitisation to address these issues. Unlike lossless compression, extreme digitisation introduces additional noise into the data. We present optimal 1, 2 and 3 bit digitisation schemes and determine the degradation in temperature and polarisation power spectra caused by applying this process to the TOD. We find that 3 bit digitisation has a percent-level contribution to the map noise level. This is impressive considering that it would reduce the data volume by an order of magnitude. We argue that extreme digitisation is a promising strategy for upcoming experiments.
\end{abstract}

\keywords{ cosmic background radiation --- polarization --- data compression}
\section{Introduction}
\label{sec:intro}

Observations of the Cosmic Microwave Background (CMB) have played a key role in physics since 1964. Current and future CMB experiments will continue to deliver new insights by studying the temperature and polarisation information contained in the CMB. These will put tight constraints on cosmological models. The most prominent science goal is the discovery of the imprint of inflationary gravitational waves. Additionally, studies of CMB lensing and the Sunyaev-Zeldovich (SZ) effects will open up access to unprecedented insight. Moreover, through probing the relativistic number of species, the helium fraction and the neutrino mass sum, CMB experiments are a valuable counter-part to ground-based particle physics experiments.

The outstanding contribution of CMB science to modern physics has demanded a high standard of data analysis. The CMB community has developed a variety of compression and computational techniques to manage the increasing influx of data, while maximising the science output. These include the compression of time-ordered data (TOD) into maps, bandpower estimation and the pseudo $C_l$ method.

A growing hurdle for experiments at remote locations are the transmission limitations of satellite links. Space-based experiments have employed a combination of lossless and lossy compression techniques, including reduced bits in the TOD. Antarctica-based experiments that transmit a portion of their data via a satellite link have downsampled their data in the past to meet their telemetry requirements. They have yet to exploit few bit digitisation of the TOD. As we approach the next generation of ground-based experiments, Stage-4, and the launch of a new generation of space-based missions (liteBIRD, PIXIE, COrE+), we must treat the transmission bottleneck carefully. Without a review of the current compression techniques employed we are sure to lose information.

In this work we present the method of extreme digitisation, which compresses a rich digital input signal and compresses it into a few bits. We apply extreme digitisation to the TOD and detail its effect on temperature and polarisation power spectra. We find that an optimal 3-bit digitisation scheme adds as little as $< 2\%$ to the map noise level. While the digtisation schemes described here are primarily laid out for ground-based experiments, the results we present here should also be considered by future space-based missions, which inevitably must incorporate lossy compression.

This work is structured as follows. We detail the arising challenges in handling large TOD in \S\ref{subsec:problem}. We subsequently formulate extreme digitisation in \S\ref{subsec:extremedigitisation} and lay out the framework used to test its performance in \S\ref{subsec:method}. The details of the power spectrum estimation used are laid out in \S\ref{subsec:psestimation}. We continue by presenting the noise induced through the dicitisation process in \S\ref{subsec:additionalnoise} and summarise our findings in \S\ref{sec:conclusions}. 

%CMB is great; One reason is that there is a history of compression and computational techniques that reduce the load of large datasets. ie maps; bandpowers; pseudo-cls.

%satellites have also reduced bits on TOD; ground based haven't had to yet

%however as we discuss building ever larger arrays at remote sites, we are starting to be limited: spt example.

%in this work we present digitisation for ground-based cmb polarisation measurements.
%teaser results


%The outline of this paper is as follows. 
%We present the digisation schemes in \S\ref{sec:dig}, and their performance in \S\ref{sec:results}
%We summarize our findings in \S\ref{sec:conclusions}. 

\section{Digitisation}
\label{sec:dig}

\subsection{Problem}
\label{subsec:problem}

%Data influx + Transmission

The science goals of upcoming CMB experiments naturally lead to a large influx of data. To achieve the targeted sensitivity longer observations with more detectors are needed. In fact the number of detectors of ground-based experiments has been following a Moore's law like trend, doubling approximately every 2 years. This directly translates into an exponential growth in data volume.

The best observations sites for CMB measurements are at remote locations: in space and in the Antarctica. Experiments at these locations depend on satellite transmission. The next generation of ground-based experiments will aim to collect 2 million detector years of data. An experiment contributing to Stage-4 at the South Pole will face a data influx of $\sim \mathrm{O}(10)\mathrm{Tb/d}$. However, the current transmission allocation for SPT3G is at $150\mathrm{Gb/d}$, which will likely only see a moderate increase in coming years. The transmission bottleneck is currently overcome by recovering the full data on hard drives with some latency and by transmitting a downsampled version of the data. The downsampling process loses high frequency information. Going into Stage-4 we anticipate that compression rates for transmission must increase by an order of magnitude. Continued use of downsampling will narrow the information window decisively - prohibiting high multipole moment science to be carried out on the transmitted dataset. This also means that any potential faults or errors in the experiment that only become visible in high frequencies will go unnoticed for longer.

Future space-based missions aim to exceed the detector count of Planck by at least an order of magnitude. It is questionable whether their telemetry specifications will allow for transmission of the data with Planck-style compression. Methods of storing large amounts of data on upcoming satellites will likely be prohibited by financial decisions: the amount of storage space required becomes financially relevant at the scales targeted. Missions will likely be left to design their own compression algorithms which will incorporate a combination of lossless and lossy compression techniques.

%Planning

Beyond transmission challenges, mission planning is becoming exceedingly difficult. As noted by (S4 science book) a full simulation of TOD over the entire parameter space of detection scenarios for numerous set-ups is the desired way to decide on Stage-4 configurations. Space-based missions must aim to carry out a similar analysis to optimise their science output. Given the shear size of TOD expected, this is not possible. We must rely on different planning strategies or aim to reduce the size of the TOD in order to maximise the productivity of planning and development stages and guarantee scientific excellence.

% Analysis

Operations on the TOD, such as noise-removal or map-making are a vital part of CMB data analysis. While we have not experienced the limitations of the accessible computational assets, the exponential growth of CMB data makes its analysis increasingly expensive. 

%why is it worth considering

Extreme Digitisation would tackle the challenges mentioned above by reducing the size of the TOD by an order of magnitude. Together with already established lossless compression techniques (e.g. FLAC, run-length coding, Huffman coding, etc.) this will directly tackle transmission hurdles. While it needs to be investigated to what extent existing algorithms can be carried over, extreme digitisation has the potential of solving planning and analysis problems.

Other science areas have demonstrated that extreme digitisation is a valuable compression technique. Jenet and Anderson (cite) explored the application of such compression to radio pulsar timing measurements with success. Recently Clearwater et al. (in prep.) have investigated the advantages of using 1 and 2 bit data when searching for continuous gravitational waves using the Laser Interferometer Gravitational-wave Observatory (LIGO).


\subsection{Extreme Digitisation}
\label{subsec:extremedigitisation}

% rephrase these, basically off Max1978

Digitisation is a lossy compression technique. However the induced noise depends on the number of bits used, the digitisation thresholds, and the output levels chosen. To minimise the noise induced through this process one must know the nature of the input signal. A theoretical framework to obtain these levels was laid out by Max in 1978. We review the key aspects of his work relevant for us below.

Digitisation discretises an input signal by sorting it into $N$ appropriate ranges, such that an input between $x_i$ and $x_{i+1}$ produces an output at $y_i$. A digitisation scheme is described by the number of ranges, $N$, the endpoints of these ranges, $x_i$, and the output levels, $y_i$. Conventionally one chooses $x_{1} = -\infty$ and $x_{N+1} = \infty$. In order to quantify the performance of a given digitisation scheme we define the distortion as

\[ D = \left\langle  \left( s - \hat{s} \right)^2 \right\rangle \]

where $s$ is the input and $\hat{s}$ the output signal. For an input signal that has at least some stochastic element to it we introduce the input amplitude probability density $p(x)$. This allows us to rewrite the above as

\[ D = \sum_{i = 1}^N \int_{x_i}^{x_{i+1}} \left(x-y_i\right)^2 p(x) dx \]

Seeing as we wish to minimise the distortion we differentiate the above with respect to $x_i$ and $y_i$ and set the derivatives to zero. We obtain the two equations

\begin{equation} \label{eq:distderiv1}
\frac{\partial D}{\partial x_i} = \left(x_i-y_{i-1}\right)^2 p(x_i) - \left(x_i - y_i\right)^2 p(x_i) = 0
\end{equation}

\begin{equation} \label{eq:distderiv2}
\frac{\partial D}{\partial x_j} = -2 \int_{x_i}^{x_{i+1}} \left( x-y_i \right) p(x) dx = 0
\end{equation}

Rearranging equation \ref{eq:distderiv1} we deduce

\begin{equation} \label{eq:digitequalspacecondition}
x_i = \frac{y_i+y_{i+1}}{2}
\end{equation}

which informs us that an output level $y_i$ must lie halfway between its delimiting thresholds $x_i$ and $x_{i+1}$. We gain an additional condition from equation \ref{eq:distderiv2}

\begin{equation} \label{eq:digitareacondition}
\int_{x_i}^{x_{i+1}} \left( x-y_i \right) p(x) dx = 0
\end{equation}

This implies that we should choose $y_i$, such that it halves the area underneath $p(x)$ in the interval from $x_i$ to $x_{i+1}$.

To progress further we have to make an assumption about the distribution of input signals, $p(x)$. For our purposes we assume that CMB observations operate at low signal to noise. Furthermore we assume that the noise profile is Gaussian white noise\footnote{Please see the conclusion for a discussion of the effect of more realistic noise profiles.}, i.e. $p(x) = 1/\sqrt{2\pi} e^{-x^2/2}$. Given this assumption we can solve the problem using a numerical iterative procedure. One begins by picking $y_1$ and calculating the remaining $x_i$'s and $y_i$'s using equation \ref{eq:digitequalspacecondition}. Afterwards one observes whether this choice of values satisfy the conditions given by equation \ref{eq:digitareacondition}. If that is the case, the $x_i$'s and $y_i$'s were chosen appropriately.

This was carried out by Max. We incorporate his results by formulating the multi-level functions we use for our 1, 2 and 3 bit digitisation process. Given an input signal $s(t)$ a digitisation scheme using $N$ bits returns the output $\hat{s}_N(t)$. For 1 bit digitisation we apply the function

\[ \hat{s}_1(t) = \left\{ \begin{array}{lr}
1, & \text{for } s(t) > 0\\
-1, & \text{for } s(t) \leq 0
\end{array} \right. \]

to the TOD. For 2 bit digitisation we apply the four-level function

\[ \hat{s}_2(t) = \left\{ \begin{array}{rl}
1.51 \sigma, & \text{for } s(t) \geq 0.9816 \sigma\\
0.4528 \sigma, & \text{for } 0 \leq s(t) < 0.9816 \sigma\\
-0.4528 \sigma, & \text{for } 0.9816 \sigma \leq s(t) < 0\\
-1.51 \sigma, & \text{for } 0.9816 \sigma < s(t)\\
\end{array} \right. \]

where $\sigma$ is the standard deviation of the input signal. Finally the optimal 3 bit digitisation is described by the eight-level function

\[ \hat{s}_3(t) = \left\{ \begin{array}{rl}
2.152 \sigma, & \text{for } s(t) \geq 1.748 \sigma\\
1.344 \sigma, & \text{for } 1.05 \sigma \leq s(t) < 1.748 \sigma\\
0.756 \sigma, & \text{for } 0.501 \sigma \leq s(t) < 1.05 \sigma\\
0.245 \sigma, & \text{for } 0 \leq s(t) < 0.501 \sigma\\
-0.245 \sigma, & \text{for } 0.501 \sigma \leq s(t) < 0\\
-0.756 \sigma, & \text{for } 1.05 \sigma \leq s(t) < 0.501 \sigma\\
-1.344 \sigma, & \text{for } 1.748 \sigma \leq s(t) < 1.05 \sigma\\
-2.152 \sigma, & \text{for } 1.748 \sigma < s(t)\\
\end{array} \right. \]

Other digitisation schemes can be thought of, which place the digitisation thresholds and output levels in a different way. However, given the assumptions made the schemes derived above are optimal. Additionally, they are simple enough to be easily implemented computationally.

\subsection{Methods}
\label{subsec:method}

To investigate the performance of the derived digitisation schemes we simulate many scans over CMB template maps at the timestream level. Each scan is performed by a single detector. We obtain control maps that use 64bit TOD and maps that have undergone 1, 2 and 3 bit digitisation at the timestream level. We calculate the temperature and polarisation power spectra of each map and determine the additional noise induced through the extreme digitisation process.

To create the template maps we use the healpix framework and the wealth of support available for it. We generate a realisation of I, Q and U maps based on the results of Planck 2015. The key cosmological parameters are summarised in table \ref{tab:inputcosparams}.

We simulate observing a $\sim 600 \mathrm{deg^2}$ patch of the sky. To do so we perform a number of constant elevation scans (CES), equally spaced in declination (DEC). We repeat the observation strategy $100$ times with a slight offset in right ascension (RA) and DEC each time, such that all pixels within the patch are hit approximately uniformly. The speed at which we sweep across the survey area is adjusted to to produce a desired number of hits per pixel (hpp) in the output maps.

While performing each CES the pixels being targeted are determined. The corresponding values from the template maps are then accessed and added to realisations of the detector noise of appropriate length. We assume the detector noise to be Gaussian white noise.

At this point we apply the digitisation schemes to the TOD. We compress the timestream into maps by averaging all hits falling into the same pixel. We produce 12 CMB maps in total: three control maps (I, Q, U) and nine maps with three each obtained from each digitisation scheme.

This process is carried out 6 times, producing maps with different numbers of hpp. The simulation parameters are summarised in table \ref{tab:modelparams}. The calculation outlined above has considerable computational requirements if we want to reach up to $\sim 10^8$ hpp. To carry out this simulation we make use of parallelisation and the computing facilities provided by NERSC.

\begin{table}[tbh]
\begin{center}
\caption{\label{tab:inputcosparams} Input Cosmological Parameters}
\small
\begin{tabular}{l | c}
Parameter & Planck 2015\\
\hline\\[-1.5ex]
$100\theta_{MC}$ & $ 1.04086 \pm 0.00048$\\
$\Omega_b h^2$ & $  0.02222 \pm 0.00023$\\
$\Omega_c h^2$ & $ 0.1199 \pm 0.0022$\\
$H_0$ & $67.26 \pm 0.98$\\
$n_s$ & $0.9652 \pm 0.0062$\\
$\Omega_m$ & $0.316 \pm 0.014$\\
$\sigma_8$ & $0.830 \pm 0.015 $\\
$\tau$ & $0.078 \pm 0.019$\\
$10^9 A_s e^{-2\tau}$ & $1.881 \pm 0.014 $\\
\end{tabular}
\tablecomments{ 
Cosmological parameters used to create the template maps. Taken from Planck 2015: Cosmological Parameters.
} \normalsize
\end{center}
\end{table}

\begin{table}[tbh] % transpose and include hpp!
\begin{center}
\caption{\label{tab:modelparams} Assumed Survey Parameters}
\small
\begin{tabular}{l | c}
Parameter & Value\\
\hline \\[-1.5ex]
$\mathrm{NSIDE}$ &$4096$\\
$f_{\mathrm{readout}} [\mathrm{Hz}]$ &$200$\\
$f_{{\mathrm{sky}}}$ & $\sim 0.014$\\
$\sigma^{\mathrm{T}}_{\mathrm{det}} [\mathrm{\mu K \sqrt{s}}]$ &$500$\\
$\sigma^{\mathrm{Pol}}_{\mathrm{det}} [\mathrm{\mu K \sqrt{s}}]$ & $\sqrt{2}\times500$\\
\multirow{2}{*}{$\mathrm{hpp}$} & $\left(1, 10, 100 \right) \times 800,$\\
& $\left( 1, 10, 100 \right) \times 1024000$ \\
\end{tabular}
\tablecomments{ 
Parameters used in the simulated observation. The RA speed is adjusted to match the desired hpp.
} \normalsize
\end{center}
\end{table}



\section{Results}
\label{sec:results}

\subsection{Power Spectrum Estimation}
\label{subsec:psestimation}

We use PolSpice to compute the TT, EE and BB power spectra of the reconstructed maps I, Q, U maps. When doing so we apodise the observed skypatch using a cosine mask to minimise cut sky effects on the spectra. Please see table \ref{tab:polspiceparams} for an overview of the parameters used in this process.

\begin{table}[tbh]
\begin{center}
\caption{\label{tab:polspiceparams} PolSpice Parameters}
\small
\begin{tabular}{c c c c}
weights & $\sigma_{\mathrm{APOD}} [\mathrm{deg}]$ & apodistation type & polarisation \\
\hline
cosine mask & $\sqrt{600}/2$ & cosine & Yes \\

\end{tabular}
\tablecomments{ 
Parameters used when calling PolSpice to calculate the power spectra. Remaining parameters have been left at their default value.
} \normalsize
\end{center}
\end{table}

To normalise the the obtained power spectra we focus on the $\sim 10^8 \mathrm{hpp}$ maps. We normalise each power spectrum originating from few bit TOD against its corresponding control counterpart. We place a lower limit on the normalisation window in multipole space by considering the size of the observed patch with

\[ l = \frac{2}{\pi} \left( \frac{32400}{600} \right) \approx 35 \]

where we have rounded up to the next largest integer. We find the upper bound on the normalisation window by demanding that

\[ \frac{C_l^{\mathrm{S}}}{C_l^{\mathrm{N}}} \geq 10 \]

where $C_l^{\mathrm{S}}$ is the power spectrum of the input template maps and $C_l^{\mathrm{N}}$ the detector noise level.

we close the normalisation window the first instance the above condition is violated. The normalisation constants for each digitisation scheme and channel obtained through this way are then applied to all lower hpp simulations. A sample of the obtained power spectra are shown in figure \ref{fig:powspeczoom}.

\begin{figure}[htb]\centering
\includegraphics[width=0.5\textwidth,clip]{Plots/ttzoom.png}
\includegraphics[width=0.5\textwidth,clip]{Plots/eezoom.png}
\includegraphics[width=0.5\textwidth,clip]{Plots/bbzoom.png}
  \caption[Current ]{
   Reconstructed $\mathrm{TT}$, $\mathrm{EE}$, and $\mathrm{BB}$ power spectra. These originate from an observation with $102400000 \mathrm{hpp}$. We observe that digitisation appears to add some constant to the noise level. As one would expect $3 \mathrm{Bit}$ digitisation performs the best, followed by $2 \mathrm{Bit}$- and finally $1 \mathrm{Bit}$ digitisation.
\label{fig:powspeczoom}
}
\end{figure}


\subsection{Additional Noise}
\label{subsec:additionalnoise}

To quantify how much the quality of the power spectra suffers from the digitisation process we compare the map noise levels inferred from the power spectra. These are put into the context of the map noise levels deduced from the control power spectra. We formulate

\begin{equation} \label{eq:extramapnoise}
\frac{\Delta \sigma}{\sigma} = \frac{\sigma_{\mathrm{map}}^{\mathrm{D}}-\sigma_{\mathrm{map}}^{\mathrm{C}}}{\sigma_{\mathrm{map}}^{\mathrm{C}}}
\end{equation}

where $\sigma_{\mathrm{map}}^{\mathrm{C}}$ is the map noise level of the control power spectra and $\sigma_{\mathrm{map}}^{\mathrm{D}}$ is the map noise level obtained from the power spectra originating from extremely digitised TOD. We assume that the digitisation process results in adding a constant noise term to each power spectrum. If we are dominated by noise we can write

\[ C_l^D \approx C_l^N + C_l^X \]

Here $C_l^D$ is the power spectrum originating from a digitised timestream, $C_l^N$ is the detector noise level and $C_l^X$ the additional noise induced through digitisation. We now progress equation \ref{eq:extramapnoise} to

\[\frac{\Delta \sigma}{\sigma} = \sqrt{\frac{C_l^D}{C_l^{N}}} - 1  = \sqrt{\frac{C_l^N + C_l^X}{C_l^{N}}} - 1 = \sqrt{1 + \frac{C_l^X}{C_l^{N}}} - 1  \]

The value of $l$ at which we can safely assume to be noise dominated and apply the above framework varies between simulated observations of different hpp and temperature and polarisation power spectra. The range considered involves any datapoints at multipole moments larger than the last point at which the detector noise is at least an order of magnitude larger than the template power spectrum, i.e.

\[ \frac{C_l^{\mathrm{N}}}{C_l^{\mathrm{S}}} \geq 10 \]

Before analysing $C_l^X/C_l^N$ we rebin the power spectra to $\Delta l = 123$. This guarantees that the points in the noise tail are independent of one another, allowing us to extract an uncertainty for the above quantity. Plots for $C_l^X/C_l^N$ are shown in figure \ref{fig:clxcln}.

\begin{figure}[htb]\centering
\includegraphics[width=0.5\textwidth,clip]{Plots/ttratio.png}
\includegraphics[width=0.5\textwidth,clip]{Plots/eeratio.png}
\includegraphics[width=0.5\textwidth,clip]{Plots/bbratio.png}
  \caption[Current ]{
  Calculated ratio of $C_l^X/C_l^N$ of the rebinned $\mathrm{TT}$, $\mathrm{EE}$, and $\mathrm{BB}$ power spectra. The vertical black line indicates from which point onwards data is used to calculate the equivalent noise level. The above power spectra originate from a $80000\mathrm{hpp}$ map. Increasing the hpp in the map shifts the plateau to the right.
\label{fig:clxcln}
}
\end{figure}


The deduced additional noise for 1, 2 and 3 bit digitisation schemes are shown with respect to the hits per pixel in the maps in figure \ref{fig:extranoise}. We would like to point out three key results. Firstly, 3 Bit digitisation performs the best, followed by 2 Bit- and finally 1 Bit digitisation. This is what we expect, given that with each additional bit we retain more information. Secondly, for a fixed detector noise level the deterioration of the map quality scales in the same fashion as the number of hpp. We see that $\Delta \sigma / \sigma$ is independent of the hpp in the maps. Lastly, the added noise levels are astonishingly low. Keeping in mind that CMB detector sensitivity improves in steps of order of magnitude every few years, adding an extra percent-level noise term does not deteriorate the results appreciably. This is impressive, given that the use of an optimal 3 bit digitisation scheme will save approximately an order of magnitude in TOD volume.

\begin{figure}[htb]\centering
\includegraphics[width=0.5\textwidth,clip]{Plots/addednoise.png}
  \caption[Current ]{
  Addition to the map noise level due to digitisation. 
           \label{fig:extranoise}
  }
\end{figure}

Extreme digitisation is a viable lossy compression technique when dealing with low signal to noise, large datasets with a signal that is slowly varying with respect to the sampling rate. Under these conditions we have a locally flat, low signal, on top of which we add many noise realisations. In the limit of many samples these allow us to reconstruct the signal well. Furthermore a small signal will prevent saturation of the output, i.e. the inability of the digitised output to communicate any information where in the range $\infty$ to $x_N$ an input signal lies.

%Extreme digitisation is a viable lossy compression technique when dealing with large volume, low signal to noise, slowly varying datasets. Making these assumptions the signal is small and constant for a considerable number of datapoints. For such an interval many samplings of the signal plus noise result in producing appropriate numbers in each output bin. The addition of more bits helps by classifying the size of the noise more accurately and hence approaching an improved estimate for the value of the signal faster. It is necessary to remain in the low signal to noise regime, as otherwise a dominant signal would saturate the digitisation scheme and for a given scheme only values in the range $\pm |N_{\mathrm{hits}}y_N|$ can be represented, given that we have $N_{\mathrm{hits}}$ datapoints. We must demand the data to be slowly varying such that many datapoints can be used to reconstruct a single significant step of the input signal. Finally a large data volume is clearly favourable for the reasons illustrated above: it allows for repeated sampling and therefore more accurate reconstruction of the weak signal.

%\begin{table*}[tbh]
%\begin{center}
%\caption{\label{tab:experiments} Assumed survey parameters}
%\small
%\begin{tabular}{l || c c c c c }
%Experiment & Sky coverage & Polarized Noise level  & 1/$f$ knee & Beam FWHM \\
%& &($\mu$K-arcmin)&&(arcmin.)\\
%\hline
%\tiny \\ \small
%CMB Stage III & & & & \\
%~~~~~SPT-3G & 6\% & 3.0 & 200 & 1.2 \\
%~~~~~Simons Array & 36\% & 9.5 & 200 & 3.5 \\ 
%\tiny \\ \small
%\hline
%CMB Stage IV & 55\% & 1.3 & 100 & 4.0 \\
%\end{tabular}
%\tablecomments{ 
%Key numbers about the planned stage III and IV experiments. 
%The sky coverage percentages are after galactic cuts. 
%Unless otherwise noted,  the Fisher matrix forecasts in this work use these numbers. 
%All forecasts also allow for beam and calibration uncertainties as noted in the text. 
%} \normalsize
%\end{center}
%\end{table*}

\section{Conclusions}
\label{sec:conclusions}

In this work we have motivated the investigation of extreme digitisation as a technique in combating arising data challenges in CMB data analysis. The reduction of the TOD by an order of magnitude directly addresses the issues in data transmission faced by remote location observations. Benefits in mission planning and data analysis are possible.

We have derived a set of optimal digitisation schemes and presented the level at which the induce noise into the temperate and polarisation power spectra. We find that an optimal 3 bit digitisation adds as little as $<2\%$ to the map noise level.

Future work investigating this compression technique must aim to understand the nature of the induced noise better. It is of great value to find the higher statistical moments, i.e. skewness and kurtosis of the added noise term. For this an analysis of the performance of cluster-finding algorithms on the digitised datasets is useful.

It should be laid out how the results changes when moving to a more realistic noise profile. We do not expect this to alter the practicality of our results - even if a different noise profile doubles the additional percentage to the map noise level extreme digitisation is still practical. Ideas on how to deal with 1/f noise, e.g. chunking of the data before applying extreme digitisation have already been investigated by Planck. These thoughts should be considered when designing the compression schemes of future space-based CMB missions, which will be unable to recover their full data with latency, but rely entirely on the transmitted data.


\acknowledgments % thank patrick for discussion/starting point? Andrew?

We thank the \changed{referee as well as} Srinivasan Raghunathan and Federico Bianchini for valuable feedback on the manuscript. 
We acknowledge support from an Australian Research Council Future Fellowship (FT150100074), and also from the University of Melbourne. 
This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. 
We acknowledge the use of the Legacy Archive for Microwave Background Data Analysis (LAMBDA). Support for LAMBDA is provided by the NASA Office of Space Science.


%\bibliography{digitisation}


\end{document}
